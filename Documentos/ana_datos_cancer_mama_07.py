# -*- coding: utf-8 -*-
"""Ana_Datos_Cancer_Mama_07.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12SI76f4XQD2AhOnOP5ex-Bwx43SvLQII

#**Clasificaci√≥n del Cancer de Mama**

## Carga y Extracci√≥n de Datos
"""

# Usando wget para descargar el archivo y nombrarlo localmente 'data_cancer.csv'
!wget -O data_cancer.csv 'https://raw.githubusercontent.com/GuidoRiosCiaffaroni/EstudioCancerDeMama/main/DataSet/data.csv'

# Cargar el archivo local
import pandas as pd
df = pd.read_csv('data_cancer.csv')

print("¬°Descarga y carga exitosa usando wget!")
#print(df.head())

"""###Carga de Librerias"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Pandas (pd):**  Manejo y manipulaci√≥n de datos estructurados (DataFrames, Series)

**NumPy (np):** C√°lculos num√©ricos y operaciones matem√°ticas con arrays

**Matplotlib (plt):** Biblioteca base para crear visualizaciones personalizadas

**Seaborn (sns):** Capa avanzada para visualizaci√≥n estad√≠stica con est√©tica mejorada

###Carga de Archivo
"""

df = pd.read_csv('/content/data_cancer.csv')

print("DataFrame original:")
print(df)

"""###Descripci√≥n del C√≥digo
El c√≥digo es muy conciso y realiza dos tareas fundamentales en el an√°lisis de datos usando la librer√≠a Pandas de Python:

**Carga de Datos:**

df = pd.read_csv('/content/data_cancer.csv'): Utiliza la funci√≥n read_csv de Pandas (denotada por pd) para leer el contenido del archivo data_cancer.csv ubicado en la ruta temporal /content/ de Google Colab. El resultado de esta lectura se almacena en una estructura de datos llamada DataFrame de Pandas, asignada a la variable df.

**Visualizaci√≥n Inicial:**

* print("DataFrame original:"): Simplemente imprime una etiqueta para el resultado.

* print(df): Imprime el contenido completo del DataFrame (df), mostrando las primeras y √∫ltimas filas, junto con todas las columnas. Esto es una verificaci√≥n inicial crucial.

###Interpretaci√≥n de los Resultados
Los resultados muestran que la carga del archivo fue exitosa y ofrecen una primera visi√≥n de la estructura y el contenido del dataset.

1. *Estructura del Dataset*
* Filas y Columnas: Se indica [569 rows x 33 columns]. Esto significa que el dataset contiene 569 registros (muestras de pacientes o biopsias) y 33 columnas (variables/caracter√≠sticas).

* Encabezados (...): Las filas .. y las columnas ... indican que el DataFrame es demasiado grande para mostrarse completamente en la consola, por lo que Pandas omite las filas y columnas intermedias.

2. *Contenido Clave* (An√°lisis de Columna)
El dataset es, con alta probabilidad, el famoso "Breast Cancer Wisconsin (Diagnostic) Dataset". Las columnas visibles confirman esto:

* id: Un identificador √∫nico para cada muestra/paciente.

* diagnosis: Esta es la variable objetivo (la etiqueta que queremos predecir). Contiene valores categ√≥ricos:

* M: Malignant (Maligno - Cancer√≠geno).
* B: Benign (Benigno - No cancer√≠geno).

* Variables de Caracter√≠sticas: La mayor√≠a de las columnas describen caracter√≠sticas de la masa celular, derivadas de im√°genes digitales de aspirados con aguja fina (FNA). Hay 10 caracter√≠sticas b√°sicas, y cada una se mide en tres variantes (lo que da un total de 30 caracter√≠sticas):
  * _mean (Media): El valor promedio de la caracter√≠stica (ej: radius_mean
  
  * _se (Error Est√°ndar): La variabilidad o error est√°ndar de la caracter√≠stica (no visible completamente en el extracto, pero inferido
  
  * _worst (Peor/Mayor): El valor m√°s grande o "peor" de la caracter√≠stica (ej: radius_worst).

Ejemplos de Caracter√≠sticas:

* radius: Radio del n√∫cleo.

* texture: Textura de la superficie (desviaci√≥n est√°ndar de los valores de escala de grises).

* perimeter: Per√≠metro.

* area: √Årea.

* smoothness: Suavidad de los radios.

* compactness, concavity, concave points, etc.


3. *Observaci√≥n de Datos Faltantes* (Limpieza de Datos)
Unnamed: 32: Esta columna aparece al final y est√° llena de valores NaN (Not a Number).

*Interpretaci√≥n:* Esta columna no contiene datos √∫tiles y probablemente es un residuo de la exportaci√≥n del archivo CSV. En el siguiente paso del an√°lisis de datos, deber√≠a ser eliminada para evitar problemas en el modelado.

## An√°lisis Exploratorio

###Ver los primeros 3 datos de la tabla
"""

df.head(3)

"""###Ver los ultimos 3 filas"""

df.tail(3)

"""###An√°lisis del DataFrame - Dataset de C√°ncer de Mama

####Estructura General
* Filas mostradas: 3 (primeras observaciones)
* Columnas totales: 33
* Contexto: Dataset m√©dico relacionado con diagn√≥stico de c√°ncer de mama

####Variables Clave Identificadas
Variables de Identificaci√≥n
* id: Identificador √∫nico de cada caso
* diagnosis: Variable objetivo (M = Maligno, B = Benigno)

####Caracter√≠sticas M√©dicas (medidas en promedios)
* radius_mean: Radio promedio del n√∫cleo celular
* texture_mean: Textura promedio (variaci√≥n de intensidad)
* perimeter_mean: Per√≠metro promedio
* area_mean: √Årea promedio
* smoothness_mean: Suavidad promedio
* compactness_mean: Compacidad promedio
* concavity_mean: Concavidad promedio
* concave points_mean: Puntos c√≥ncavos promedio

####Patr√≥n de Nomenclatura
Las caracter√≠sticas siguen un patr√≥n:

* [caracter√≠stica]_mean: Valores promedios
* [caracter√≠stica]_worst: Valores m√°s severos (ej: texture_worst, perimeter_worst)

###Problemas Detectados
1. Columna Problem√°tica
* Unnamed: 32: Columna sin nombre, posiblemente artefacto de importaci√≥n
* Evidencia: Todos los valores son NaN en las primeras 3 filas

2. Composici√≥n del Dataset
Todos los casos mostrados: Diagn√≥stico "M" (Maligno)
* Variables num√©ricas: Mediciones continuas de caracter√≠sticas celulares
* Variable categ√≥rica: diagnosis (M/B)

###Ver cuantos registros tiene el data set
"""

df.shape

"""##PROCESO

###Comprobar tipo de datos
"""

df.dtypes

"""### An√°lisis de Tipos de Datos del DataFrame

Distribuci√≥n de Tipos de Datos
Resumen General
Total de columnas: 33

* Columnas num√©ricas: 32 (97%)
* Columnas categ√≥ricas: 1 (3%)
* Precisi√≥n predominante: float64 (mediciones continuas)

Desglose por Categor√≠as
1. Variable Identificadora
  * id: int64
  * Identificador √∫nico de casos
  * Valor entero sin decimales

2. Variable Objetivo (Target)
  * diagnosis: object
  * √önica variable categ√≥rica del dataset
  * Codificaci√≥n esperada: "M" (Maligno), "B" (Benigno)

3. Variables de Caracter√≠sticas M√©dicas (31 columnas)
  * Todas son float64 - Representan mediciones continuas de alta precisi√≥n:
  * Medidas de Tendencia Central (_mean)
  * radius_mean, texture_mean, perimeter_mean, area_mean
  * smoothness_mean, compactness_mean, concavity_mean, concave points_mean
  * symmetry_mean, fractal_dimension_mean
  * Medidas de Error Est√°ndar (_se)
  * radius_se, texture_se, perimeter_se, area_se
  * smoothness_se, compactness_se, concavity_se, concave points_se
  * symmetry_se, fractal_dimension_se
  * Medidas de Valores Extremos (_worst)
  * radius_worst, texture_worst, perimeter_worst, area_worst
  * smoothness_worst, compactness_worst, concavity_worst, concave points_worst
  * symmetry_worst, fractal_dimension_worst

4. Columna Problem√°tica
* Unnamed: 32: float64
* Tipo inconsistente con su contenido (valores NaN)
* Confirmaci√≥n para eliminaci√≥n
* Evaluaci√≥n de Calidad de Datos

### Fortalezas
* Consistencia tipol√≥gica: Todas las mediciones m√©dicas en float64
* Precisi√≥n adecuada: float64 permite alta precisi√≥n para medidas biom√©dicas
* Estructura predecible: Patr√≥n claro en nomenclatura de variables

###Eliminar Columas Unmamed: 32
"""

# Drop the 'Unnamed: 32' column
df_cleaned = df.drop(columns=['Unnamed: 32'])

print("\nDataFrame despu√©s de eliminar la columna 'Unnamed: 32':")
print(df_cleaned.info())

"""###Resultado de la Limpieza

####Estructura Final del DataFrame
* Filas totales: 569 observaciones
* Columnas finales: 32 (reducidas de 33)
* Memoria utilizada: 142.4+ KB

####Distribuci√≥n de Tipos de Datos Mejorada
* float64: 30 columnas (93.8%)
* int64: 1 columna (3.1%) - id
* object: 1 columna (3.1%) - diagnosis

Calidad de Datos Lograda
Integridad Total

* Todas las columnas: 569 non-null
* Completitud: 100% de los datos presentes
* Sin valores nulos en todo el dataset

ANTES (33 columnas)                 DESPU√âS (32 columnas)
- 1 columna problem√°tica            - Sin columnas redundantes
- 1 columna con NaN                 - Todas las columnas con datos completos
- Memoria: ~145+ KB                 - Memoria: 142.4+ KB (2% de reducci√≥n)

### Variables por Categor√≠a
1. Identificaci√≥n (1)
id (int64): Identificador √∫nico

2. Variable Objetivo (1)
diagnosis (object): Clasificaci√≥n M/B

3. Caracter√≠sticas M√©dicas (30)
Todas float64, organizadas en tres grupos:

Medidas Centrales (_mean): 10 features
Errores Est√°ndar (_se): 10 features
Valores Extremos (_worst): 10 features

Evaluaci√≥n del Proceso
√âxitos de la Limpieza

‚úÖ Columna problem√°tica eliminada

‚úÖ Integridad de datos preservada (0 valores nulos)

‚úÖ Estructura coherente mantenida

‚úÖ Reducci√≥n m√≠nima de memoria

###Cambiar Nombre de las columnas
"""

df_cleaned.columns = [
    'id',
    'diagnostico',
    'radio_medio',
    'textura_media',
    'perimetro_medio',
    'area_media',
    'suavidad_media',
    'compacidad_media',
    'concavidad_media',
    'puntos_concavos_medios',
    'simetria_media',
    'dimension_fractal_media',
    'radio_error_estandar',
    'textura_error_estandar',
    'perimetro_error_estandar',
    'area_error_estandar',
    'suavidad_error_estandar',
    'compacidad_error_estandar',
    'concavidad_error_estandar',
    'puntos_concavos_error_estandar',
    'simetria_error_estandar',
    'dimension_fractal_error_estandar',
    'radio_maximo',
    'textura_maxima',
    'perimetro_maximo',
    'area_maxima',
    'suavidad_maxima',
    'compacidad_maxima',
    'concavidad_maxima',
    'puntos_concavos_maximos',
    'simetria_maxima',
    'dimension_fractal_maxima'
]
print(df_cleaned)

"""### Descripci√≥n del C√≥digo
1. Objetivo del C√≥digo
El c√≥digo tiene como objetivo renombrar todas las columnas del DataFrame df_cleaned (que asumimos es una versi√≥n del dataset de C√°ncer de Mama de Wisconsin donde se elimin√≥ la columna Unnamed: 32 y quiz√°s otras limpiezas menores) a nombres descriptivos en espa√±ol.

2. Mec√°nica del Renombramiento
df_cleaned.columns = [...]: Esta l√≠nea asigna una nueva lista de strings directamente al atributo .columns del DataFrame df_cleaned.

La lista de 32 strings proporciona un nuevo nombre a cada una de las 32 columnas del DataFrame, en el mismo orden en que aparecen originalmente (por ejemplo, radius_mean se convierte en radio_medio).

Prop√≥sito: La renombraci√≥n es fundamental para la legibilidad del c√≥digo futuro. Trabajar con nombres como radio_medio en lugar de radius_mean (o la abreviatura original concave points_mean) hace que el c√≥digo de an√°lisis y modelado sea m√°s intuitivo, especialmente cuando se trabaja en espa√±ol.

### Interpretaci√≥n de los Resultados
El resultado muestra el DataFrame df_cleaned despu√©s de aplicar los nuevos nombres de columna.

1. Verificaci√≥n de la Renombraci√≥n
√âxito: Las columnas originales (id, diagnosis, radius_mean, etc.) han sido reemplazadas por los nuevos nombres descriptivos en espa√±ol: id, diagnostico, radio_medio, textura_media, perimetro_medio, etc.

* Coherencia: Se confirma que la lista de nombres ten√≠a la longitud correcta (32), ya que el resultado muestra [569 rows x 32 columns], lo que indica que no se perdieron ni se agregaron columnas.

2. Interpretaci√≥n de los Datos (Variables)
El dataset sigue representando mediciones de n√∫cleos celulares, pero ahora la estructura de las variables se entiende de forma inmediata:

Grupo de Variable	Prefijo en Espa√±ol	Descripci√≥n
Variable Objetivo	diagnostico	Clase (M: Maligno / B: Benigno). Es la variable a predecir.
Estad√≠stico Descriptivo	..._medio	Representa la media aritm√©tica de 10 caracter√≠sticas nucleares (radio, textura, √°rea, etc.).
Estad√≠stico de Variabilidad	..._error_estandar	Representa el error est√°ndar de las 10 caracter√≠sticas, indicando la dispersi√≥n o variabilidad de la medici√≥n.
Estad√≠stico Extremo	..._maximo	Representa el "peor" o valor m√°ximo para cada una de las 10 caracter√≠sticas, que suele ser la medida m√°s importante para indicar la malignidad.

###Ver si hay Valores Nulos
"""

df_cleaned.isnull().sum()

"""###Ver Valores At√≠picos"""

# Selecciona las variables num√©ricas
variables_numericas = df_cleaned.select_dtypes(include=['int64', 'float64']).columns

# Define el n√∫mero de columnas para el layout de los subplots
num_cols = 4
num_rows = (len(variables_numericas) + num_cols - 1) // num_cols

# Crea la figura y los subplots
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols * 4, num_rows * 3))
axes = axes.flatten()

# Itera sobre cada variable num√©rica y dibuja un box plot
for i, col in enumerate(variables_numericas):
    sns.boxplot(x=col, data=df_cleaned, ax=axes[i])
    axes[i].set_title(f'Box Plot de {col}')
    axes[i].set_xlabel('')

# Oculta los ejes no utilizados
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()

"""###Interpretaci√≥n Espec√≠fica del Dataset de C√°ncer de Mama

Las 30 variables (excluyendo el id) son caracter√≠sticas calculadas a partir de una imagen digitalizada de un aspirado con aguja fina (FNA) de una masa mamaria. Se calculan tres valores para 10 caracter√≠sticas nucleares: la media, el error est√°ndar (variabilidad dentro de la muestra) y el "peor" o m√°ximo (el valor m√°s grande).

**1. Variables de Tendencia Central (_media)**

Estas variables representan la caracter√≠stica promedio de las c√©lulas.

| Variable | Interpretaci√≥n | Observaci√≥n Clave del Box Plot |
|----------|----------------|--------------------------------|
| `radio_media`, `perimetro_medio`, `area_media` | Medidas del tama√±o de la c√©lula. | Alta dispersi√≥n y muchos outliers positivos. Esto indica que existen tumores significativamente m√°s grandes que el promedio en la muestra. |
| `textura_media` | Desviaci√≥n est√°ndar de los niveles de gris (rugosidad de la superficie). | Distribuci√≥n amplia. |
| `concavidad_media` | Severidad de las porciones c√≥ncavas del contorno. | Sesgo positivo y outliers. Es esperable, ya que la concavidad tiende a ser baja en c√©lulas benignas y alta en malignas. |


**2. Variables de Variabilidad Celular (_error_estandar)**

Estas variables indican cu√°n inconsistente es la caracter√≠stica a trav√©s de las c√©lulas dentro de una misma muestra.


| Variable | Interpretaci√≥n | Observaci√≥n Clave del Box Plot |
|----------|----------------|--------------------------------|
| `area_error_estandar`, `perimetro_error_estandar` | Variabilidad del tama√±o. | Extrema presencia de outliers positivos y alta dispersi√≥n. Los tumores m√°s agresivos suelen tener una gran variabilidad en el tama√±o celular (anisonucleosis). |
| `suavidad_error_estandar`, `compacticidad_error_estandar` | Variabilidad de la forma. | Cajas peque√±as, pero a√∫n con presencia de outliers. |
| **Conclusi√≥n General:** Las variables de error est√°ndar son vitales para la clasificaci√≥n, ya que capturan la heterogeneidad celular. Su alta presencia de outliers positivos sugiere que la heterogeneidad es un rasgo extremo en ciertos casos. | | |


**3. Variables de Extremo (_maximo)**
Estas variables representan los valores m√°s extremos o "peores" de las caracter√≠sticas observadas.


| Variable | Interpretaci√≥n | Observaci√≥n Clave del Box Plot |
|----------|----------------|--------------------------------|
| `radio_maximo`, `perimetro_maximo`, `area_maxima` | Medidas del tama√±o de las c√©lulas m√°s grandes en la muestra. | Muestran la mayor dispersi√≥n y una gran cantidad de outliers positivos. El rango de estos valores es significativamente m√°s amplio que sus contrapartes _media. |
| `concavidad_maxima`, `puntos_concavos_maximos` | Severidad m√°xima de la forma c√≥ncava. | Fuerte sesgo positivo y gran dispersi√≥n. El valor m√°ximo de estas caracter√≠sticas es un predictor muy fuerte de malignidad. |
| **Conclusi√≥n General:** Las variables _maximo son, por dise√±o, las m√°s propensas a ser fuertemente sesgadas y contener outliers, ya que capturan el l√≠mite superior de la patolog√≠a observada. | | |

### Implicaciones para el Preprocesamiento de Datos
El an√°lisis de estos Box Plots revela desaf√≠os y oportunidades clave para el modelado predictivo:

*Outliers Informativos:*

Los numerosos outliers positivos no deben eliminarse sin cuidado. En este contexto, un outlier (ej. un tumor con un radio_maximo excepcionalmente grande) a menudo representa el caso m√°s severo de c√°ncer maligno. Si se eliminan, el modelo perder√° informaci√≥n crucial para clasificar correctamente los casos m√°s extremos.

Necesidad de Escalamiento:

Existe una enorme diferencia de escalas entre las variables. Por ejemplo, area_media (cientos o miles) versus suavidad_media (d√©cimas).

Acci√≥n: Se requiere aplicar escalamiento (ej. StandardScaler o MinMaxScaler) antes de entrenar modelos sensibles a la escala, como K-Nearest Neighbors (KNN), Regresi√≥n Log√≠stica o Redes Neuronales.

Manejo de Sesgo:

La mayor√≠a de las variables, especialmente las de _error_estandar y _maximo, muestran un fuerte sesgo positivo (cola larga a la derecha).

Acci√≥n: Se puede intentar una transformaci√≥n logar√≠tmica o de ra√≠z cuadrada en variables clave como area_media, area_maxima, concavidad_maxima para reducir el sesgo y mejorar el rendimiento de modelos que asumen una distribuci√≥n normal.

Variable ID:

La variable id es un identificador √∫nico. Se debe eliminar del conjunto de entrenamiento, ya que no tiene valor predictivo y solo introduce ruido.

###Contabilizar Valores Atipicos
"""

# Suponiendo que 'df_cleaned' es tu DataFrame
# Selecciona solo las columnas num√©ricas
variables_numericas = df_cleaned.select_dtypes(include=['int64', 'float64']).columns

# Diccionario para almacenar el conteo de outliers por columna
outliers_count = {}

# Itera sobre cada columna num√©rica
for col in variables_numericas:
    # Calcula el primer y tercer cuartil (Q1 y Q3)
    Q1 = df_cleaned[col].quantile(0.25)
    Q3 = df_cleaned[col].quantile(0.75)

    # Calcula el Rango Intercuart√≠lico (IQR)
    IQR = Q3 - Q1

    # Define los l√≠mites para detectar outliers
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR

    # Encuentra los outliers
    # Se consideran outliers los valores que est√°n por debajo del l√≠mite inferior
    # o por encima del l√≠mite superior
    outliers = df_cleaned[(df_cleaned[col] < limite_inferior) | (df_cleaned[col] > limite_superior)]

    # Cuenta el n√∫mero de outliers
    outliers_count[col] = len(outliers)

# Imprime los resultados
print("Conteo de valores at√≠picos (outliers) por columna:")
for col, count in outliers_count.items():
    print(f"Columna '{col}': {count} outliers")

# (Opcional) Puedes convertir el diccionario a un DataFrame para una mejor visualizaci√≥n
outliers_df = pd.DataFrame(list(outliers_count.items()), columns=['Variable', 'Cantidad de Outliers'])
print("\nResumen en formato de tabla:")
print(outliers_df)

"""###Ver M√≠nimo y M√°ximos de las Variable Numericas"""

# Selecciona todas las columnas de tipo num√©rico (int64 y float64)
variables_numericas = df_cleaned.select_dtypes(include=['int64', 'float64']).columns

# Calcula el rango (m√≠nimo y m√°ximo) para todas estas variables
rango_numericas = df_cleaned[variables_numericas].describe().loc[['min', 'max']]

print(rango_numericas)

"""###Conteo de Variables Categoricas"""

# Selecciona todas las columnas de tipo 'object' o 'category'
variables_categoricas = df.select_dtypes(include=['object', 'category']).columns

print("N√∫mero de categor√≠as √∫nicas por variable categ√≥rica:")
# Itera sobre cada variable categ√≥rica y muestra el conteo de categor√≠as √∫nicas
for col in variables_categoricas:
    print(f"- {col}: {df[col].nunique()} categor√≠as √∫nicas")

"""###GRAFICOS HISTOGRAMAS Y DISPERSI√ìN"""

variables_individuales = ['radio_medio', 'textura_media', 'area_media', 'suavidad_media', 'simetria_media']

plt.figure(figsize=(15,10))
for i, var in enumerate(variables_individuales, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df_cleaned[var], bins=30, kde=True, color='steelblue')
    plt.title(f'Distribuci√≥n de {var.replace("_", " ").capitalize()}')
    plt.xlabel(var.replace("_", " ").capitalize())
    plt.ylabel('Frecuencia')
plt.tight_layout()
plt.show()

"""### Interpretaci√≥n de los Histogramas (Variables Medias)
Los histogramas nos muestran la forma de la distribuci√≥n de cada caracter√≠stica, ayud√°ndonos a identificar el sesgo, la curtosis y la multimodalidad.

1. Variables con Sesgo Positivo Marcado (Skewness Positivo)
En estas distribuciones, la cola se extiende hacia la derecha, lo que indica que la mayor√≠a de los valores se concentran en el rango inferior, pero existen valores extremos altos.

| Variable | Observaci√≥n en el Histograma | Implicaci√≥n Estad√≠stica y Contextual |
|----------|------------------------------|--------------------------------------|
| Distribuci√≥n de Area Media | Claramente sesgada a la derecha. La moda (pico m√°s alto) se encuentra en valores bajos, pero la distribuci√≥n se extiende mucho hacia valores altos (1000 a 2500). | El √°rea del n√∫cleo celular es una medida directa del tama√±o. El fuerte sesgo sugiere que la mayor√≠a de los tumores tienen un √°rea relativamente peque√±a, mientras que los casos m√°s severos y avanzados muestran un √°rea celular excepcionalmente grande. |
| Distribuci√≥n de Radio Medio | Sesgo positivo evidente, aunque menos extremo que el √°rea. El pico se encuentra alrededor de 12-14. | El radio est√° altamente correlacionado con el √°rea. Su sesgo es similar, indicando que hay una mayor frecuencia de radios peque√±os, pero una cola de radios grandes. |

2. Variables con Distribuci√≥n m√°s Sim√©trica (Cercana a Normalidad)
Estas distribuciones se acercan m√°s a la forma de campana, aunque no son perfectamente normales.

| Variable | Observaci√≥n en el Histograma | Implicaci√≥n Estad√≠stica y Contextual |
|----------|------------------------------|--------------------------------------|
| Distribuci√≥n de Textura Media | Muestra una forma m√°s sim√©trica y est√° centrada (pico) alrededor de 20-22. | La textura (irregularidad de los niveles de gris) es menos dependiente del tama√±o absoluto del tumor. Parece seguir una distribuci√≥n m√°s uniforme en la poblaci√≥n estudiada. |
| Distribuci√≥n de Simetr√≠a Media | Parece ser la m√°s sim√©trica de las cinco, con un pico claro. | La simetr√≠a es una caracter√≠stica de la forma. El clustering de valores indica que la mayor√≠a de las c√©lulas tienden a tener un nivel de simetr√≠a similar, aunque con una ligera cola a la derecha. |
| Distribuci√≥n de Suavidad Media | Similar a la simetr√≠a, muestra una distribuci√≥n relativamente sim√©trica con un pico claro. | La suavidad (variaci√≥n local de la longitud del radio) tiende a estar bien agrupada. |


### Conclusiones y Preprocesamiento Recomendado
Este an√°lisis de distribuci√≥n confirma las observaciones hechas con los Box Plots, enfoc√°ndose en la forma de los datos

*1.- Transformaci√≥n Requerida:* Las variables area_media y radio_medio necesitan una transformaci√≥n matem√°tica (como la transformaci√≥n logar√≠tmica $\log(x)$ o $\log(1+x)$) para mitigar su fuerte sesgo positivo. Esto es esencial para mejorar el rendimiento de los modelos que asumen una distribuci√≥n normal.


*2.- Multimodalidad:* Si bien no es dr√°stica, algunas variables (como radio_medio o textura_media) muestran ligeras "jorobas" o baches en el KDE que podr√≠an insinuar una distribuci√≥n bimodal (dos picos).

* *Hip√≥tesis Contextual:* En este conjunto de datos, la bimodalidad es a menudo una consecuencia de la variable de clasificaci√≥n (Maligno vs. Benigno). Es decir, los tumores benignos forman una distribuci√≥n (pico bajo) y los malignos forman otra (pico alto), lo que se ve combinado en el histograma general.


"""

variables_duales = ['radio_medio', 'textura_media', 'area_media', 'compacidad_media', 'concavidad_media']

plt.figure(figsize=(15,10))
for i, var in enumerate(variables_duales, 1):
    plt.subplot(2, 3, i)
    sns.histplot(data=df_cleaned, x=var, hue='diagnostico', bins=30, kde=True)
    plt.title(f'Distribuci√≥n de {var.replace("_", " ").capitalize()} por Diagn√≥stico')
    plt.xlabel(var.replace("_", " ").capitalize())
    plt.ylabel('Frecuencia')
plt.tight_layout()
plt.show()

"""### Interpretaci√≥n de Histogramas por Diagn√≥stico
El c√≥digo separa la distribuci√≥n de cada caracter√≠stica entre las dos clases de la variable objetivo diagnostico (asumiendo que son Benigno y Maligno).

1. Variables con Alto Poder Discriminatorio
Estas variables muestran una separaci√≥n clara entre las distribuciones de las dos clases. Cuanto menor sea la superposici√≥n, mejor ser√° la caracter√≠stica para predecir el diagn√≥stico.

| Variable | Observaci√≥n Clave | Implicaci√≥n Predictiva |
|----------|-------------------|------------------------|
| Distribuci√≥n de Radio Medio | La separaci√≥n es muy n√≠tida. El diagn√≥stico Benigno se concentra en valores bajos (pico cerca de 10-12) y el Maligno en valores m√°s altos (pico cerca de 17). La superposici√≥n es m√≠nima. | Poder Predictivo Muy Alto. Es una de las caracter√≠sticas m√°s importantes para diferenciar tumores. |
| Distribuci√≥n de Area Media | La separaci√≥n es igualmente fuerte y clara. Los tumores Malignos tienen una area_media sustancialmente mayor que los Benignos, como se evidencia por el pico desplazado a la derecha. | Poder Predictivo Muy Alto. Confirma que el tama√±o es un factor determinante en la malignidad. |
| Distribuci√≥n de Concavidad Media | Muestra una buena separaci√≥n. Los tumores Malignos tienen una concavidad notablemente mayor (m√°s irregularidad en el contorno) que los Benignos. | Alto Poder Predictivo. La complejidad de la forma celular es un predictor fuerte. |

2. Variables con Poder Discriminatorio Moderado
Estas variables muestran una superposici√≥n considerable entre las dos distribuciones, lo que significa que el valor de la caracter√≠stica por s√≠ solo no es suficiente para clasificar el 100% de los casos.

| Variable | Observaci√≥n Clave | Implicaci√≥n Predictiva |
|----------|-------------------|------------------------|
| Distribuci√≥n de Textura Media | Las distribuciones se solapan considerablemente. El pico Maligno est√° solo ligeramente desplazado a la derecha (valores m√°s altos de textura, lo que indica m√°s rugosidad) con respecto al pico Benigno. | Poder Predictivo Moderado. La rugosidad (textura) es √∫til, pero hay muchos casos donde tumores Benignos y Malignos comparten valores de textura similares. |
| Distribuci√≥n de Compacidad Media | Muestra cierta separaci√≥n, pero con una superposici√≥n significativa. El pico Maligno est√° m√°s alto que el Benigno. | Poder Predictivo Moderado. La compacidad es informativa, pero no tan definitoria como el radio o el √°rea. |

### Conclusi√≥n y Estrategia de Modelado
Conclusi√≥n Principal
Los histogramas por diagn√≥stico confirman que la bimodalidad observada en los histogramas univariados se debe a la variable de diagn√≥stico. Las caracter√≠sticas relacionadas con el tama√±o (radio_medio, area_media) son los mejores predictores individuales del diagn√≥stico, debido a su m√≠nima superposici√≥n.

Implicaciones para el Preprocesamiento y Modelado

1.- Transformaci√≥n y Sesgo: Las distribuciones de las caracter√≠sticas Malignas (por ejemplo, area_media) siguen siendo fuertemente sesgadas a la derecha. Se mantiene la recomendaci√≥n de aplicar una transformaci√≥n logar√≠tmica a estas variables para normalizar su distribuci√≥n dentro de la clase maligna, lo que puede mejorar la eficacia de modelos basados en la distancia.

2.- Selecci√≥n de Caracter√≠sticas (Feature Selection): Las variables con m√≠nima superposici√≥n (Radio y √Årea) son excelentes candidatas para ser usadas en los modelos. Sin embargo, dado que radio_medio, perimetro_medio y area_media est√°n altamente correlacionadas (todas miden el tama√±o), se debe tener cuidado con la multicolinealidad al usar las tres a la vez.

3.- Estrategia de Clasificaci√≥n: La naturaleza de los datos, con distribuciones claramente separadas, sugiere que modelos de clasificaci√≥n como la Regresi√≥n Log√≠stica, M√°quinas de Vectores de Soporte (SVM), o √Årboles de Decisi√≥n/Random Forest ser√°n muy efectivos.
"""

pares_dispersion = [
    ('radio_medio', 'area_media'),
    ('textura_media', 'suavidad_media'),
    ('compacidad_media', 'concavidad_media'),
    ('radio_medio', 'perimetro_medio'),
    ('simetria_media', 'concavidad_media')
]

plt.figure(figsize=(15,10))
for i, (x, y) in enumerate(pares_dispersion, 1):
    plt.subplot(2, 3, i)
    sns.scatterplot(data=df_cleaned, x=x, y=y, hue='diagnostico', alpha=0.7)
    plt.title(f'{x.replace("_", " ").capitalize()} vs {y.replace("_", " ").capitalize()}')
    plt.xlabel(x.replace("_", " ").capitalize())
    plt.ylabel(y.replace("_", " ").capitalize())
plt.tight_layout()
plt.show()

"""üìà Interpretaci√≥n de los Gr√°ficos de Dispersi√≥n por Diagn√≥stico
Los gr√°ficos de dispersi√≥n con coloraci√≥n por diagn√≥stico (hue='diagnostico') son cruciales para identificar:

*.- Multicolinealidad: El grado de correlaci√≥n lineal entre las variables $X$ e $Y$.

*.- Separabilidad: Si la combinaci√≥n de ambas variables facilita la distinci√≥n entre las clases (Benigno/Maligno).

1. Variables Altamente Correlacionadas (Problema de Multicolinealidad)
Una correlaci√≥n alta indica que ambas variables miden esencialmente la misma caracter√≠stica subyacente. Los puntos se agrupan firmemente a lo largo de una l√≠nea o curva.


**Observaci√≥n:** Se observa una relaci√≥n casi perfecta y no lineal (curva) entre radio_medio y area_media. Dado que el √°rea de un c√≠rculo es proporcional al radio al cuadrado ($\text{√Årea} \propto \text{Radio}^2$), esta relaci√≥n curva es esperada.

**Impacto:** Multicolinealidad Extrema. Es un error incluir ambas variables en un modelo lineal (como Regresi√≥n Log√≠stica), ya que la informaci√≥n es redundante. Es mejor elegir una de ellas (o una versi√≥n transformada) o combinarlas.

Radio Medio vs. Per√≠metro Medio

**Observaci√≥n:** Se observa una relaci√≥n lineal casi perfecta. El per√≠metro de un c√≠rculo es proporcional al radio ($\text{Per√≠metro} \propto \text{Radio}$).
**Impacto:** Multicolinealidad Extrema. Similar al caso anterior, estas variables son redundantes.

2. Variables con Correlaci√≥n Moderada y Patrones de Separaci√≥n

Estas gr√°ficas muestran una tendencia, pero con mayor dispersi√≥n, y revelan c√≥mo las clases se distribuyen en el plano bidimensional.

Compacidad Media vs. Concavidad Media

**Observaci√≥n:** Existe una correlaci√≥n positiva fuerte a moderada. A medida que aumenta la compacidad, tambi√©n lo hace la concavidad.

**Separaci√≥n:** La clase Maligna se encuentra en la regi√≥n superior derecha del gr√°fico (alta compacidad y alta concavidad), mientras que la clase Benigna se agrupa en la regi√≥n inferior izquierda (baja compacidad y baja concavidad). La separaci√≥n es buena, con una superposici√≥n en la zona media-baja.

**Textura Media vs. Suavidad Media**

**Observaci√≥n:** Se aprecia una correlaci√≥n muy d√©bil o nula entre textura_media y suavidad_media en el conjunto de datos general. Los puntos est√°n dispersos en el plano.

**Separaci√≥n:**

* Los tumores Benignos se agrupan en el rango de baja suavidad y baja a media textura.

* Los tumores Malignos est√°n dispersos en un rango m√°s amplio, tendiendo a tener mayor textura y mayor suavidad que los benignos, pero la separaci√≥n no es tan clara como con las variables de tama√±o.

**Simetr√≠a Media vs. Concavidad Media**

* Observaci√≥n: Correlaci√≥n positiva d√©bil a moderada. El aumento de la concavidad se relaciona con una ligera tendencia al aumento de la simetr√≠a.

* Separaci√≥n: La concavidad_media sigue siendo el principal eje de separaci√≥n. Los malignos tienen alta concavidad, independientemente de la simetr√≠a (dentro de su rango). La simetr√≠a no a√±ade mucho poder discriminatorio a la concavidad.

Conclusiones Clave para el Modelado

1.- Alta Multicolinealidad: Las variables de tama√±o (radio_medio, perimetro_medio, area_media) son altamente redundantes y no deben incluirse juntas en modelos lineales para evitar problemas de estabilidad y de interpretaci√≥n de coeficientes.

**Acci√≥n:** Seleccionar solo una de las tres variables de tama√±o, o usar An√°lisis de Componentes Principales (PCA) para combinar su informaci√≥n.

2.- Poder Predictivo de Combinaciones: Las combinaciones de variables que son moderadamente correlacionadas entre s√≠, pero que tienen una fuerte relaci√≥n con el diagn√≥stico (ej., compacidad_media vs. concavidad_media), ofrecen un plano bidimensional donde la frontera de decisi√≥n entre las clases es m√°s clara.

3.- Variables Complementarias: La combinaci√≥n de una variable de tama√±o/forma (radio, concavidad) con una variable de textura/suavidad suele ser m√°s robusta, ya que capturan diferentes dimensiones de la patolog√≠a.

"""



"""###Ver Correlaci√≥n"""

print(df.select_dtypes(include=np.number).corr())

"""###Distribucion de la variable diagnostico"""

# Distribuci√≥n de la variable 'diagnosis'
print("Distribuci√≥n de 'diagnosis' (0=Benigno, 1=Maligno):")
print(df['diagnosis'].value_counts())

plt.figure(figsize=(6, 4))
sns.countplot(x='diagnosis', data=df)
plt.title('Distribuci√≥n de Diagn√≥stico')
plt.xticks(ticks=[0, 1], labels=['Benigno', 'Maligno'])
plt.show()

"""### An√°lisis de la Distribuci√≥n de la Variable Objetivo (diagnosis)
El c√≥digo tiene como objetivo visualizar y cuantificar la distribuci√≥n de la variable objetivo, diagnosis (Diagn√≥stico), que es una variable categ√≥rica binaria (Clasificaci√≥n).

1. Cuantificaci√≥n de Clases (Resultado del value_counts())

Asumiendo que el conjunto de datos es el de C√°ncer de Mama de Wisconsin (569 observaciones), los valores de la tabla de frecuencias que se obtendr√≠an son aproximadamente:

* Diagn√≥stico Benigno (0): Aproximadamente 357 observaciones.
* Diagn√≥stico Maligno (1): Aproximadamente 212 observaciones.

2. Interpretaci√≥n del Gr√°fico de Barras (Count Plot)
El gr√°fico de barras ilustra visualmente la cuenta de cada clase:

* Clase Mayoritaria (Benigno): El tumor Benigno es la clase m√°s frecuente, con una barra visiblemente m√°s alta.
* Clase Minoritaria (Maligno): El tumor Maligno es la clase menos frecuente.

3. Conclusi√≥n: Problema de Clasificaci√≥n Desbalanceado

El an√°lisis estad√≠stico de la distribuci√≥n de diagnosis revela que se trata de un problema de clasificaci√≥n desbalanceado (o "clase sesgada").

* La proporci√≥n aproximada es de 63% Benigno a 37% Maligno.

üö® Implicaci√≥n Cr√≠tica para el Modelado

El desbalance de clases tiene una implicaci√≥n directa y cr√≠tica para el entrenamiento de su modelo:

* M√©tricas de Evaluaci√≥n Enga√±osas: Si utiliza la precisi√≥n (accuracy) como m√©trica principal, un modelo ingenuo podr√≠a simplemente predecir siempre "Benigno" y a√∫n as√≠ lograr una precisi√≥n de aproximadamente el 63% (357/569), lo que ser√≠a inaceptable para un diagn√≥stico m√©dico.

* Sesgo del Modelo: Los modelos de Machine Learning tienden a priorizar la clase mayoritaria. Su modelo podr√≠a volverse muy bueno prediciendo tumores Benignos, pero muy pobre identificando tumores Malignos (los Falsos Negativos), lo cual es el error m√°s grave en un contexto m√©dico.

Recomendaciones de Preprocesamiento y Modelado

Para mitigar el problema del desbalance de clases, se recomienda:

1.- Cambiar las M√©tricas de Evaluaci√≥n: Enfocarse en m√©tricas que sean insensibles al desbalance, como:

* Recall (Sensibilidad): La capacidad de identificar correctamente los tumores Malignos (verdaderos positivos).

* F1-Score: La media arm√≥nica de precisi√≥n y recall.

* √Årea bajo la curva ROC (AUC-ROC): La mejor m√©trica general para evaluar modelos binarios en datos desbalanceados.

2.- T√©cnicas de Balanceo de Datos: Considerar aplicar t√©cnicas de remuestreo (sampling) si el rendimiento es pobre:

* Oversampling: Aumentar la clase minoritaria (ej. con SMOTE).

* Undersampling: Reducir la clase mayoritaria.

* Ajuste de Pesos de Clase: Usar el par√°metro class_weight='balanced' en los modelos (ej. Regresi√≥n Log√≠stica, Random Forest) para que el modelo penalice m√°s severamente las predicciones incorrectas en la clase minoritaria.

### Mapa de calor

> A√±adir blockquote
"""

# Calcular la matriz de correlaci√≥n
correlacion = df_cleaned.corr(numeric_only=True)

# Visualizar la matriz de correlaci√≥n como un mapa de calor
plt.figure(figsize=(15, 12))
sns.heatmap(correlacion, annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlaci√≥n')
plt.show()

# Tambi√©n puedes ver la correlaci√≥n de cada variable con la variable 'diagnostico'
# First, we need to encode the 'diagnostico' column to numerical values to calculate its correlation.
df_encoded = df_cleaned.copy()
df_encoded['diagnostico_encoded'] = df_encoded['diagnostico'].map({'M': 1, 'B': 0})

# Now calculate the correlation with the encoded diagnosis column
correlacion_with_diagnosis = df_encoded.corr(numeric_only=True)['diagnostico_encoded'].sort_values(ascending=False)

print("\nCorrelaci√≥n con la variable 'diagnostico' (0=Benigno, 1=Maligno):")
print(correlacion_with_diagnosis)

"""###Interpretaci√≥n del Mapa de Calor (Matriz de Correlaci√≥n)
El mapa de calor  revela las relaciones lineales entre todas las variables num√©ricas. Los colores rojo brillante indican una correlaci√≥n positiva fuerte ($\approx 1.0$), y los colores azul oscuro indican una correlaci√≥n negativa fuerte ($\approx -1.0$).

1. Bloques de Alta Multicolinealidad (Correlaci√≥n Positiva Fuerte)
Existen tres bloques principales de variables que est√°n extremadamente correlacionadas entre s√≠. Esto confirma los hallazgos de los scatter plots:

* Bloque 1: Dimensiones de Tama√±o (Media):
  * radio_media, perimetro_medio, area_media: Muestran una correlaci√≥n cercana a 1.0. Esto es esperado, ya que son medidas intr√≠nsecamente ligadas al tama√±o del n√∫cleo celular.

* Bloque 2: Dimensiones de Tama√±o (Peor/M√°ximo):
  * radio_maximo, perimetro_maximo, area_maxima: Tambi√©n muestran una correlaci√≥n cercana a 1.0.

* Bloque 3: Irregularidad (Concavidad y Puntos C√≥ncavos):
  * concavidad_media con puntos_concavos_medios, y sus contrapartes _maximo y _error_estandar: Muestran una alta correlaci√≥n, ya que una mayor concavidad generalmente implica m√°s puntos c√≥ncavos.

‚ö†Ô∏è Implicaci√≥n para el Modelado (Multicolinealidad): En modelos lineales (Regresi√≥n Log√≠stica, etc.), debe seleccionarse solo una variable de cada bloque (ej., solo radio_media, solo radio_maximo, y solo concavidad_media) o usar An√°lisis de Componentes Principales (PCA) para crear variables ortogonales, de lo contrario, el modelo ser√° inestable y los coeficientes ininterpretables.

2. Correlaciones Inter-Bloques (Correlaci√≥n Positiva Fuerte)
* Las variables radio_media est√°n fuertemente correlacionadas con radio_maximo (y sus contrapartes perimetro/area). Esto significa que el tama√±o promedio est√° altamente relacionado con el tama√±o m√°ximo.
* La compacidad_media y concavidad_media tambi√©n muestran una correlaci√≥n significativa, como se vio en los scatter plots.

Correlaci√≥n con la Variable Objetivo (diagnostico_encoded)

Esta tabla es la m√©trica m√°s directa del poder predictivo lineal de cada caracter√≠stica. Los valores m√°s cercanos a $+1$ o $-1$ son los m√°s relevantes.

| Variable | Correlaci√≥n (œÅ) | Interpretaci√≥n |
|----------|-----------------|----------------|
| radio_maximo | 0.78 | M√ÅS ALTA Correlaci√≥n. Un radio m√°ximo m√°s grande tiene la correlaci√≥n lineal positiva m√°s fuerte con el diagn√≥stico Maligno (1). |
| perimetro_maximo | 0.74 | Muy fuerte. Crecimiento del per√≠metro m√°ximo asociado a malignidad. |
| area_maxima | 0.73 | Muy fuerte. Gran √°rea m√°xima asociada a malignidad. |
| puntos_concavos_maximos | 0.71 | Fuerte. Mayor cantidad de puntos c√≥ncavos en el peor caso asociado a malignidad. |
| concavidad_maxima | 0.70 | Fuerte. Mayor concavidad m√°xima asociada a malignidad. |
| radio_medio | 0.65 | Fuerte. El radio promedio es un predictor s√≥lido. |
| area_media | 0.63 | Fuerte. |
| perimetro_medio | 0.61 | Fuerte. |
| id | 0.04 | Correlaci√≥n Nula. Confirma que esta variable debe ser descartada. |
| dimension_fractal_error_estandar | -0.01 | Correlaci√≥n Nula/Muy D√©bil. Es la variable menos predictiva. |

Conclusiones Clave de la Correlaci√≥n con el Diagn√≥stico
1. El Peor Caso Predice Mejor: Las variables con el sufijo _maximo (radio_maximo, perimetro_maximo, area_maxima) son consistentemente los mejores predictores lineales del diagn√≥stico, superando a sus contrapartes _media y _error_estandar. Esto tiene sentido cl√≠nico: la identificaci√≥n de las c√©lulas m√°s anormales o grandes (el "peor" caso) es clave para el diagn√≥stico de c√°ncer.
2. Importancia del Tama√±o y la Forma: El tama√±o (radio, √°rea, per√≠metro) y la irregularidad de la forma (concavidad, puntos_concavos) son las caracter√≠sticas m√°s importantes.
3. Variables In√∫tiles: Variables como dimension_fractal_error_estandar, simetria_error_estandar, y textura_error_estandar tienen una correlaci√≥n muy cercana a cero y probablemente no aportar√°n valor lineal al modelo.

Resumen y Estrategia de Selecci√≥n de Caracter√≠sticas
La matriz de correlaci√≥n nos permite hacer una selecci√≥n de caracter√≠sticas informada y mitigar la multicolinealidad:
1. Descarte Definitivo: Eliminar id.
2. Selecci√≥n de Representantes (Multicolinealidad): Del conjunto de variables de tama√±o (radio, perimetro, area) en sus tres versiones (_media, _error_estandar, _maximo), se recomienda elegir solo las versiones _maximo por su mayor poder predictivo.
3. Conjunto de Caracter√≠sticas Potenciales: Un buen conjunto inicial de caracter√≠sticas para el modelado ser√≠a:

* radio_maximo (representante del tama√±o)

* concavidad_maxima (representante de la forma/irregularidad)

* textura_media (predictor moderado, baja multicolinealidad con el tama√±o)

* simetria_media (para capturar aspectos diferentes de la forma)

###Se aplic√≥ una prueba t de Student
"""

import pandas as pd
from scipy.stats import ttest_ind
import numpy as np

# --- Asegurar formato correcto ---
# Use df_cleaned instead of df as it has the 'diagnostico' column
df_cleaned['diagnostico'] = df_cleaned['diagnostico'].map({'B': 'Benigno', 'M': 'Maligno'})
# If already in text, no need for this step

# Seleccionar las columnas num√©ricas
numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns

# Crear DataFrames separados por diagn√≥stico
df_benigno = df_cleaned[df_cleaned['diagnostico'] == 'Benigno']
df_maligno = df_cleaned[df_cleaned['diagnostico'] == 'Maligno']

# Crear lista para almacenar resultados
resultados = []

# Recorrer todas las variables num√©ricas y aplicar la prueba t
for col in numeric_cols:
    benigno_vals = df_benigno[col].dropna()
    maligno_vals = df_maligno[col].dropna()

    # Solo ejecutar si hay datos en ambos grupos
    if len(benigno_vals) > 1 and len(maligno_vals) > 1:
        t_stat, p_value = ttest_ind(maligno_vals, benigno_vals, equal_var=False)
        significancia = "S√≠" if p_value < 0.05 else "No"

        resultados.append({
            'Variable': col,
            't_estadistico': round(t_stat, 4),
            'valor_p': round(p_value, 4),
            'Diferencia_significativa': significancia
        })

# Convertir a DataFrame para ver resultados
tabla_resultados = pd.DataFrame(resultados)

# Mostrar resultados ordenados por valor p (menor a mayor)
tabla_resultados = tabla_resultados.sort_values(by='valor_p', ascending=True).reset_index(drop=True)

tabla_resultados

import matplotlib.pyplot as plt
import seaborn as sns

# Crear columna booleana (1 si hay diferencia significativa, 0 si no)
tabla_resultados['Significativo'] = tabla_resultados['Diferencia_significativa'].apply(lambda x: 1 if x == "S√≠" else 0)

# Configurar el tama√±o del gr√°fico
plt.figure(figsize=(12,6))

# Gr√°fico de barras ordenado por valor p
sns.barplot(
    data=tabla_resultados.sort_values(by='valor_p'),
    x='Variable',
    y='valor_p',
    hue='Diferencia_significativa',
    palette={'S√≠':'#d9534f', 'No':'#5bc0de'}
)

# L√≠nea de referencia en p = 0.05
plt.axhline(0.05, color='black', linestyle='--', label='Nivel de significancia (Œ± = 0.05)')

# T√≠tulos y etiquetas
plt.title('Resultados de pruebas t para variables num√©ricas\nComparaci√≥n entre tumores benignos y malignos', fontsize=14)
plt.xlabel('Variable', fontsize=12)
plt.ylabel('Valor p', fontsize=12)
plt.xticks(rotation=90)
plt.legend(title='Diferencia significativa')

plt.tight_layout()
plt.show()

"""### Interpretaci√≥n del Gr√°fico de Valores p (Pruebas t de Student)

El gr√°fico muestra el resultado de una prueba de hip√≥tesis (t de Student) para cada variable, donde la hip√≥tesis nula ($H_0$) es que no existe diferencia significativa en la media de la variable entre los tumores benignos y malignos.El valor p es la probabilidad de observar la diferencia de medias actual (o una m√°s extrema) si $H_0$ fuera verdadera.

1. **El Nivel de Significancia ($\alpha = 0.05$)**

    La l√≠nea de referencia en $p = 0.05$ (la l√≠nea discontinua negra) es el umbral.

* Si $p < 0.05$ (Barra Roja): Se rechaza la $H_0$. Concluimos que existe una diferencia estad√≠sticamente significativa entre las medias de la variable para los tumores benignos y malignos.

* Si $p \ge 0.05$ (Barra Azul): Se falla en rechazar la $H_0$. Concluimos que no hay suficiente evidencia para afirmar que las medias son diferentes.

2. **Variables con Diferencia Altamente Significativa** ($p \ll 0.05$)

###Analisis Regresion logistica
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split

# ==============================================================================
# 1. Preparaci√≥n de la Variable Dependiente (Y)
# ==============================================================================

COLUMNA_DIAGNOSTICO = 'diagnostico' #
VALOR_MALIGNO = 'Maligno'
VALOR_BENIGNO = 'Benigno'
# Codificaci√≥n binaria: 1 para Maligno, 0 para Benigno
df_cleaned['Y_binaria'] = df_cleaned[COLUMNA_DIAGNOSTICO].map({VALOR_MALIGNO: 1, VALOR_BENIGNO: 0})


# ==============================================================================
# 2. Selecci√≥n de Variables Predictoras (X)
# ==============================================================================

# Variables con 'Diferencia Significativa' (p < 0.05) del diagrama de barras
variables_predictoras = [
    'radio_medio',
    'textura_media',
    'perimetro_medio',
    'area_media',
    'suavidad_media',
    'compacidad_media',
    'concavidad_media',
    'puntos_concavos_medios',
    'simetria_media',
    'dimension_fractal_media',
    'radio_maximo',
    'area_maxima',
    'perimetro_maximo',
    'concavidad_maxima'

]

X = df_cleaned[variables_predictoras]
Y = df_cleaned['Y_binaria']


# ==============================================================================
# 3. Separaci√≥n de Datos (Pr√°ctica de Machine Learning)
# ==============================================================================

# 70% para entrenamiento, 30% para prueba
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)


# ==============================================================================
# 4. Modelo de Regresi√≥n Log√≠stica (Logit)
# ==============================================================================

# 4.1 A√±adir la constante (intercepto)
X_train_con_constante = sm.add_constant(X_train)

# 4.2 Crear y ajustar (entrenar) el modelo Logit
modelo_logistico = sm.Logit(Y_train, X_train_con_constante).fit()

# 4.3 Mostrar el resumen estad√≠stico
print("="*60)
print("       RESUMEN DEL MODELO DE REGRESI√ìN LOG√çSTICA (LOGIT)")
print("="*60)
print(modelo_logistico.summary())


# ==============================================================================
# 5. C√°lculo e Interpretaci√≥n de Odds Ratios
# ==============================================================================

# Calcular Odds Ratios (OR) elevando el coeficiente a la potencia de 'e' (np.exp)
odds_ratios = pd.DataFrame(
    {'Coeficiente (Log Odds)': modelo_logistico.params,
     'Odds Ratio (e^Coef)': np.exp(modelo_logistico.params),
     'P-Value': modelo_logistico.pvalues}
)

# Eliminar la constante de la tabla para enfocarse en las variables
odds_ratios = odds_ratios.drop('const')

# Ordenar por Odds Ratio para mejor visualizaci√≥n
odds_ratios = odds_ratios.sort_values(by='Odds Ratio (e^Coef)', ascending=False)

print("\n" + "="*60)
print("             AN√ÅLISIS DE ODDS RATIOS (OR) Y P-VALUES")
print("="*60)
print(odds_ratios)


print("\n--- Interpretaci√≥n R√°pida de Odds Ratios ---")
print("1. Si OR > 1.0 y P-Value < 0.05: La variable es predictora significativa de MALIGNIDAD (Y=1).")
print("2. Si OR < 1.0 y P-Value < 0.05: La variable es predictora significativa de BENIGNIDAD (Y=0).")
print("3. Si P-Value >= 0.05: La variable NO es un predictor significativo dentro del modelo.")

"""### Modelo predictivo de diagn√≥stico de c√°ncer de mama"""

# ============================================================
# üî¨ MODELO PREDICTIVO DE C√ÅNCER DE MAMA ‚Äì REGRESI√ìN LOG√çSTICA
# ============================================================

# Importar librer√≠as necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,
    classification_report, roc_curve, auc
)

# ============================================================
# 1Ô∏è‚É£ DETECCI√ìN AUTOM√ÅTICA DE COLUMNA DE DIAGN√ìSTICO
# ============================================================
print("Columnas disponibles en el dataset:")
print(df_cleaned.columns.tolist(), "\n")

# Detectar nombre de columna que contenga "diagn" (diagn√≥stico/diagnosis)
col_diagnostico = None
for col in df_cleaned.columns:
    if "diagn" in col.lower():  # busca 'diagn' en min√∫sculas
        col_diagnostico = col
        break

if col_diagnostico is None:
    raise KeyError("No se encontr√≥ una columna de diagn√≥stico (ej. 'diagnostico' o 'diagnosis')")

print(f"‚úÖ Columna de diagn√≥stico detectada autom√°ticamente: {col_diagnostico}")

# ============================================================
# 2Ô∏è‚É£ Codificar variable objetivo: 1 = Maligno, 0 = Benigno
# ============================================================
# Detecci√≥n autom√°tica de etiquetas (M/B o Maligno/Benigno)
valores_unicos = df_cleaned[col_diagnostico].unique()
print(f"Valores √∫nicos detectados: {valores_unicos}")

# Ajuste autom√°tico seg√∫n idioma o formato
if 'M' in valores_unicos or 'B' in valores_unicos:
    df_cleaned['diagnostico_cod'] = df_cleaned[col_diagnostico].map({'M': 1, 'B': 0})
else:
    df_cleaned['diagnostico_cod'] = df_cleaned[col_diagnostico].map({'Maligno': 1, 'Benigno': 0})

# ============================================================
# 3Ô∏è‚É£ Separar variables predictoras (X) y objetivo (y)
# ============================================================
X = df_cleaned.drop(columns=['id', col_diagnostico, 'diagnostico_cod'], errors='ignore')
y = df_cleaned['diagnostico_cod']

# ============================================================
# 4Ô∏è‚É£ Divisi√≥n de datos y escalado
# ============================================================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================================================
# 5Ô∏è‚É£ Entrenamiento del modelo predictivo
# ============================================================
log_model = LogisticRegression(max_iter=1000, random_state=42)
log_model.fit(X_train_scaled, y_train)

# ============================================================
# 6Ô∏è‚É£ Evaluaci√≥n del modelo
# ============================================================
y_pred = log_model.predict(X_test_scaled)
y_pred_prob = log_model.predict_proba(X_test_scaled)[:, 1]

precision = accuracy_score(y_test, y_pred)
print(f"\nüîπ Precisi√≥n del modelo: {precision*100:.2f}%\n")

print("üîπ Reporte de Clasificaci√≥n:\n")
print(classification_report(y_test, y_pred))

# ============================================================
# 7Ô∏è‚É£ Gr√°ficos de evaluaci√≥n
# ============================================================

# Matriz de confusi√≥n
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(cmap='Greens')
plt.title("Matriz de Confusi√≥n ‚Äì Regresi√≥n Log√≠stica")
plt.show()

# Curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("Tasa de falsos positivos")
plt.ylabel("Tasa de verdaderos positivos")
plt.title("Curva ROC ‚Äì Rendimiento del Modelo")
plt.legend()
plt.show()

# Importancia de variables
importances = pd.Series(log_model.coef_[0], index=X.columns)
importances.sort_values().plot(kind='barh', figsize=(8, 8), color='teal')
plt.title("Importancia de las Variables en la Predicci√≥n")
plt.show()

# Heatmap de correlaciones
# Select only numeric columns for correlation calculation
numeric_df = df_cleaned.select_dtypes(include=np.number)
plt.figure(figsize=(10,8))
sns.heatmap(numeric_df.corr(), cmap='coolwarm', annot=False)
plt.title("Mapa de Calor de Correlaciones entre Variables Num√©ricas")
plt.show()

# Histograma de Radio Medio por diagn√≥stico (if exists)
if 'radio_medio' in df_cleaned.columns:
    sns.histplot(data=df_cleaned, x='radio_medio', hue=col_diagnostico, kde=True)
    plt.title("Distribuci√≥n de Radio Medio seg√∫n Diagn√≥stico")
    plt.show()